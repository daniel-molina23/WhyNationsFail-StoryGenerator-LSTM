{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StoryGeneratorUsing_LSTMs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daniel-molina23/WhyNationsFail-StoryGenerator-LSTM/blob/main/StoryGeneratorUsing_LSTMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNsM-CzHnPq3"
      },
      "source": [
        "# from pydrive.auth import GoogleAuth\n",
        "# from pydrive.drive import GoogleDrive\n",
        "# from google.colab import auth\n",
        "# from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# # 1. Authenticate and create the PyDrive client.\n",
        "# auth.authenticate_user()\n",
        "# gauth = GoogleAuth()\n",
        "# gauth.credentials = GoogleCredentials.get_application_default()\n",
        "# drive = GoogleDrive(gauth)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMc6J0D0y_Sr"
      },
      "source": [
        "import requests\n",
        "URL = 'https://archive.org/stream/WhyNationsFailDaronAcemogluAndJamesA.Robinson1/Why+Nations+Fail+-+Daron+Acemoglu+and+James+A.+Robinson+%281%29_djvu.txt'\n",
        "page = requests.get(URL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDGT6Wqw4eMt"
      },
      "source": [
        "from lxml import html\n",
        "tree = html.fromstring(page.content)\n",
        "story = tree.xpath('//pre/text()')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ise3EQ3t0zRc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ec8deb48-61c0-48ba-91ac-98da32e539a4"
      },
      "source": [
        "print(len(story))\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXyL06u57WbO"
      },
      "source": [
        "story = story[0].replace('\\n','')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwMF8OhSIf7n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "06136a67-4f4c-417a-dee9-764080e81c27"
      },
      "source": [
        "s = 'or sailor to transport out of the colony, for his own private uses, upon pain of death.'\n",
        "index = story.find(s)\n",
        "story[index:index+len(s)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'or sailor to transport out of the colony, for his own private uses, upon pain of death.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vGDUDhF_MjA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "07326ba7-6c99-41f2-8eb3-5e9811f17438"
      },
      "source": [
        "starts = ['THE CITY OF NOGALES is cut in half by a fence. If you stand by it and look north, you\\'ll see', 'If the indigenous peoples could not be exploited, reasoned the Virginia Company, perhaps the colonists', 'What explains these major differences in poverty and prosperity and the patterns of growth? Why did', 'Finally, geographic factors are unhelpful for explaining not only the differences we see across various', 'The real reason that the Kongolese did not adopt superior technology was because they lacked any', 'The situation north of the 38th parallel was different. Kim Il-Sung, a leader of anti-Japanese', 'Critical junctures can also result in major change toward rather than away from extractive institutions.', 'Existing archaeological evidence does not allow us to reach a definitive conclusion about why the', 'In Europe, feudal institutions emerged following the collapse of central state authority. The same', 'While Britain and most of northwest Europe was crisscrossed with railways in 1870, very few penetrated the vast territory of Russia. The policy', 'THE MOLUCCAN ARCHIPELAGO in modern Indonesia is made up of three groups of islands. In', 'These states had absolutist forms of government similar to those in Europe in the same period. The', 'All this warfare and conflict not only caused major loss of life and human suffering but also put in', \"To the development economists who visited South Africa in the 1950s and '60s, when the academic\", \"Initial military success encouraged the Republic's leadership to expand France's borders, with an eye\", 'The reason that the economic and political trajectory of the South never changed, even though slavery', 'Casanare is not a poor department. On the contrary, it has the highest level of per capita income of']\n",
        "ends = ['or sailor to transport out of the colony, for his own private uses, upon pain of death.', 'The Soviet Union is an even more noteworthy example, growing rapidly between 1930 and 1970, but subsequently experiencing a rapid collapse.', 'Ottoman Empire, and it is the institutional legacy of this empire that keeps the Middle East poor today.', 'own self-interest, but first by European colonialism and then by postindependence African governments.', 'the state behind rapid economic growth, channeling credit and subsidies to firms that were successful.', 'were used to launch a process of political and economic change that paved the way for economic growth.', 'they also disappeared. Elsewhere elites seem to have gone at the same time as the divine lord.', 'stopped being coined, the urban population fell, and there was a refocusing of the state into the interior of the country and up into the highlands of modern Ethiopia.', 'construction. After 1849 he even used his power to censor discussion in the newspapers of railway development.', 'and this time with an even bigger bang and more fundamental effects on the prosperity and poverty of nations.', 'Banten, Melaka, Makassar, Pegu, and Brunei expanded rapidly, producing and exporting spices along with other products such as hardwoods.', 'the south, the trade was just as vigorous. On the Loango coast, north of the Kingdom of Kongo, Europeans sold about 50,000 guns a year.', 'of cultivating a defined piece of ground ... But the native cannot pay the master anything for his right to occupy the land.\"', \"conscription allowed the French to field large armies and develop a military advantage verging on supremacy even before Napoleon's famous military skills came on the scene.\", 'vicious circle of extractive institutions was stronger than many had expected at the time.', 'not only fought against the FARC, but also murdered innocent civilians and terrorized and displaced hundreds of thousands of people from their homes.', 'economic and political institutions, on many small differences that matter and on the very contingent path of history.']\n",
        "s = 'IN THE SUMMER OF 1945, as the Second World War was drawing to a close, the Japanese colony in'\n",
        "e = 'nation to make strides toward more inclusive institutions. In addition some luck is key, because history always unfolds in a contingent way.'\n",
        "starts_smaller = starts[4:17] # does not include 17\n",
        "starts_smaller[0] = s\n",
        "ends_smaller = ends[4:16] # does not include 16\n",
        "ends_smaller.append(e)\n",
        "\n",
        "print(len(starts), len(ends))\n",
        "clean_story = ''\n",
        "\n",
        "for i in range(len(starts_smaller)):\n",
        "  story_begins_index = story.find(starts_smaller[i])\n",
        "  story_ends_index = story.find(ends_smaller[i]) + len(ends_smaller[i])\n",
        "  if story_begins_index == -1 or story_ends_index == -1:\n",
        "    print('Oopsie on index: ', i)\n",
        "  clean_story += story[story_begins_index:story_ends_index] + ' '\n",
        "print(len(starts_smaller), len(ends_smaller))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17 17\n",
            "13 13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsdR7UYJJpiU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e1f033ba-522a-4203-dccd-2c1c406ce9b7"
      },
      "source": [
        "print(len(clean_story))\n",
        "start_c = story.find(starts_smaller[0])\n",
        "end_c = story.find(ends_smaller[12]) + len(ends_smaller[12])\n",
        "print(len(story[start_c:end_c]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "758254\n",
            "765382\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfssfJnWfUAQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "841dc90a-3426-419a-90cf-ba7b4b1d55a3"
      },
      "source": [
        "import string\n",
        "\n",
        "# perform data cleaning to find meaningful data that can be used\n",
        "def clean_data(doc):\n",
        "    doc = doc.lower()\n",
        "    tokens = doc.split() # split by whitespace\n",
        "    table = str.maketrans('','',string.punctuation)\n",
        "    tokens = [word.translate(table) for word in tokens] # remove the punctuations from all words \n",
        "    tokens = [word for word in tokens if word.isalpha()] # removes the words that contain numbers\n",
        "    return tokens\n",
        "\n",
        "cleaned_data = clean_data(clean_story)\n",
        "l = len(cleaned_data)\n",
        "cleaned_data = cleaned_data[:(l//2)]\n",
        "print(len(cleaned_data[:(l//2)]))\n",
        "print(len(cleaned_data)) # see the total number of words\n",
        "print(len(set(cleaned_data))) # see the number of unique words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60142\n",
            "60142\n",
            "6509\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTnaHVZf-QOx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624
        },
        "outputId": "1e5f00d6-7821-4e39-9270-f52e3e9f68ed"
      },
      "source": [
        "cleaned_data[1645:1680]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['that',\n",
              " 'of',\n",
              " 'the',\n",
              " 'total',\n",
              " 'population',\n",
              " 'on',\n",
              " 'the',\n",
              " 'island',\n",
              " 'of',\n",
              " 'around',\n",
              " 'almost',\n",
              " 'were',\n",
              " 'african',\n",
              " 'slaves',\n",
              " 'who',\n",
              " 'were',\n",
              " 'the',\n",
              " 'property',\n",
              " 'of',\n",
              " 'the',\n",
              " 'remaining',\n",
              " 'onethird',\n",
              " 'of',\n",
              " 'the',\n",
              " 'population',\n",
              " 'indeed',\n",
              " 'they',\n",
              " 'were',\n",
              " 'mostly',\n",
              " 'the',\n",
              " 'property',\n",
              " 'of',\n",
              " 'the',\n",
              " 'largest',\n",
              " 'sugar']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBN02mwuf1l5"
      },
      "source": [
        "#Create the training model to use the first 50 words of a phrase to predict the 51st\n",
        "length = 50 + 1\n",
        "lines = []\n",
        "\n",
        "for i in range(length, len(cleaned_data)):\n",
        "    sequence = cleaned_data[i-length:i]\n",
        "    line = ' '.join(sequence) # make into single string\n",
        "    lines.append(line)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ny3c9C7Jf6i4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "8dbe58eb-ddd3-4159-aada-da795b078696"
      },
      "source": [
        "print(lines[1])\n",
        "print(len(lines))\n",
        "print(cleaned_data[51])\n",
        "\n",
        "# Data has been cleaned, now time to use the LSTMs for next word prediction\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the summer of as the second world war was drawing to a close the japanese colony in korea began to collapse within a month of japans august unconditional surrender korea was divided at the parallel into two spheres of influence the south was administered by the united states the north by\n",
            "60091\n",
            "by\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZbs3INTf-7h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "db0eda36-535b-4ea1-e7ba-eebe3c6bda5a"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines) # converts from qualitative to quantitative data\n",
        "sequences = tokenizer.texts_to_sequences(lines)\n",
        "len(sequences[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "51"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20Iszp7h5JTQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "040be9a1-f1b6-4709-e4c0-dee9e1cb54ef"
      },
      "source": [
        "print(len(tokenizer.index_word))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6509\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZaYzRLrgm5P"
      },
      "source": [
        "sequences = np.array(sequences)\n",
        "X, y = sequences[:,:-1], sequences[:,-1] # sets X for first 50, y to 51st only\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqYT9elS4xAh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8963f703-62d9-4dc4-8338-ab863a295946"
      },
      "source": [
        "print(y[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "115\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lo4RiCWogpOP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "5ade1484-e237-4aa3-c8ab-815b4a6e12c5"
      },
      "source": [
        "print(len(X[1]))\n",
        "print(\"last, \", y[0])\n",
        "print(len(tokenizer.word_index))\n",
        "vocab_size = len(tokenizer.word_index) + 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50\n",
            "last,  115\n",
            "6509\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKQIARgUh_O6"
      },
      "source": [
        "y = to_categorical(y, num_classes=vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMj4xV9e01Cn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "a64b068b-9920-416a-f863-4710f104dedf"
      },
      "source": [
        "print(y[1232][:100]) # one hot encoding"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFnUTEecjy-U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "910477d6-3cfb-4171-b880-08e933f10554"
      },
      "source": [
        "seq_length = X.shape[1]\n",
        "seq_length"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AIRksx5j4Uy"
      },
      "source": [
        "## Data has been prepared, now we can create the LSTM model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0p1ywSshZtw"
      },
      "source": [
        "You would run the code below if you need to train the model, but that would take too long, so we'll import a model I've already trained"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8tIXKQaeGwd"
      },
      "source": [
        "# model2 = Sequential()\n",
        "# model2.add(Embedding(vocab_size, 50, input_length=seq_length)) \n",
        "# model2.add(LSTM(100, return_sequences=True))\n",
        "# model2.add(LSTM(100))\n",
        "# model2.add(Dense(100, activation='relu'))\n",
        "# model2.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "# model2.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics=['accuracy'])\n",
        "\n",
        "# model2.fit(X, y, batch_size=256, epochs=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVLZRE3AmRh0"
      },
      "source": [
        "# model2.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S14mDUjHm43u"
      },
      "source": [
        "# import pickle\n",
        "# from sklearn.externals import joblib\n",
        "\n",
        "# file2 = 'LSTM_batch256_epochs100.pkl'\n",
        "# saved_model2 = joblib.dump(model2, file2)\n",
        "\n",
        "\n",
        "# from google.colab import files\n",
        "# files.download('LSTM_batch256_epochs100.pkl')\n",
        "\n",
        "# folder_id = '1r5NgMiULgbFilcYbrsAtemuilr6kn-M-'\n",
        "\n",
        "# # get the folder id where you want to save your file\n",
        "# file = drive.CreateFile({'parents':[{u'id': folder_id}]})\n",
        "# file.SetContentFile('LSTM_batch256_epochs100.pkl')\n",
        "# file.Upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gL8FXUGYKJsK"
      },
      "source": [
        "###Bottom 2 models didn't work. Computation time takes 35 hours... Looking for **data and numerical optimization**...."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cYBsDf2j92V"
      },
      "source": [
        "# model = Sequential()\n",
        "# model.add(Embedding(vocab_size, 50, input_length=seq_length)) \n",
        "# model.add(LSTM(400, return_sequences=True))\n",
        "# model.add(Dropout(0.2)) #prevent overfitting\n",
        "# model.add(LSTM(400))\n",
        "# model.add(Dropout(0.2)) #prevent overfitting\n",
        "# model.add(Dense(100, activation='relu'))\n",
        "# model.add(Dense(vocab_size, activation='softmax'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmeNvoPNlKAY"
      },
      "source": [
        "# model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8Bvf5FHlfya"
      },
      "source": [
        "# model.fit(X, y, batch_size=100, epochs=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzFA2-WEk4GH"
      },
      "source": [
        "# model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYerdpA2nbrM"
      },
      "source": [
        "# import pickle\n",
        "# from sklearn.externals import joblib\n",
        "\n",
        "# file1 = 'LSTM_batch200_epochs50.pkl'\n",
        "# saved_model1 = joblib.dump(model, file1)\n",
        "\n",
        "\n",
        "# from google.colab import files\n",
        "# files.download(file1)\n",
        "\n",
        "# folder_id = '1r5NgMiULgbFilcYbrsAtemuilr6kn-M-'\n",
        "\n",
        "# # get the folder id where you want to save your file\n",
        "# file = drive.CreateFile({'parents':[{u'id': folder_id}]})\n",
        "# file.SetContentFile(file1)\n",
        "# file.Upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8-ozLgUyNvv"
      },
      "source": [
        "###Runnning on TPU for faster computation time! (Tensor Processing Unit). Didn't work. TPU couldn't handle the amount of memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAhI1s0Vwi_F"
      },
      "source": [
        "# import tensorflow as tf\n",
        "# # from tensorflow.python.keras.layers import Input, LSTM, Dropout, Dense, Embedding\n",
        "\n",
        "\n",
        "# def make_model():\n",
        "#     model3 = Sequential()\n",
        "#     model3.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
        "#     model3.add(LSTM(700, return_sequences=True))\n",
        "#     model3.add(Dropout(0.2))\n",
        "#     model3.add(LSTM(700))\n",
        "#     model3.add(Dropout(0.2))\n",
        "#     model3.add(Dense(100, activation='relu'))\n",
        "#     model3.add(Dense(vocab_size, activation='softmax'))\n",
        "#     model3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "#     return model3\n",
        "\n",
        "\n",
        "# training_model = make_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fc7xcJBFyHzh"
      },
      "source": [
        "# import os\n",
        "# # This address identifies the TPU we'll use when configuring TensorFlow.\n",
        "# TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "\n",
        "# resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPU_WORKER)\n",
        "# tf.config.experimental_connect_to_cluster(resolver)\n",
        "# # This is the TPU initialization code that has to be at the beginning.\n",
        "# tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "# strategy = tf.distribute.experimental.TPUStrategy(resolver)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrKliM0yyrIl"
      },
      "source": [
        "# training_model.fit(X, y, batch_size=50, epochs=10)\n",
        "# training_model.save_weights('./tpu_model.h5', overwrite=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgIVAR5uzBzp"
      },
      "source": [
        "### End of TPU run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Slr60yhy9g0"
      },
      "source": [
        "# inferencing_model = make_model(batch_size=None)\n",
        "# inferencing_model.load_weights('./tpu_model.h5')\n",
        "# inferencing_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkHZxQmWicoY"
      },
      "source": [
        "# model3 = Sequential()\n",
        "# model3.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
        "# model3.add(LSTM(700, return_sequences=True))\n",
        "# model3.add(Dropout(0.2))\n",
        "# model3.add(LSTM(700))\n",
        "# model3.add(Dropout(0.2))\n",
        "# model3.add(Dense(vocab_size, activation='softmax'))\n",
        "# model3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# model3.fit(X, y, batch_size=100, epochs=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXrwPFPUnkA-"
      },
      "source": [
        "# model3.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uLbbHhgnmkr"
      },
      "source": [
        "# import pickle\n",
        "# from sklearn.externals import joblib\n",
        "\n",
        "# file3 = 'LSTM_batch3big_layers.pkl'\n",
        "# saved_model3 = joblib.dump(model3, file3)\n",
        "\n",
        "\n",
        "# from google.colab import files\n",
        "# files.download(file3)\n",
        "\n",
        "# folder_id = '1r5NgMiULgbFilcYbrsAtemuilr6kn-M-'\n",
        "\n",
        "# # get the folder id where you want to save your file\n",
        "# file = drive.CreateFile({'parents':[{u'id': folder_id}]})\n",
        "# file.SetContentFile(file3)\n",
        "# file.Upload()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uHG1asjiEq9"
      },
      "source": [
        "##Importing a trained model..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcO0oeHbsRXU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "d596333d-985e-4b37-ebd3-84909fe59220"
      },
      "source": [
        "import pickle\n",
        "from sklearn.externals import joblib\n",
        "\n",
        "model = joblib.load('LSTM_batch256_epochs100.pkl')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwuxB8nRm1mL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "70f42b68-dbea-4985-b894-feda5985dd1b"
      },
      "source": [
        "from random import seed\n",
        "from random import randint\n",
        "\n",
        "seed()\n",
        "rand_index = randint(0,len(lines))\n",
        "print(rand_index)\n",
        "\n",
        "seed()\n",
        "rand_index2 = randint(0,len(lines))\n",
        "print(rand_index2)\n",
        "\n",
        "seed()\n",
        "rand_index3 = randint(0,len(lines))\n",
        "print(rand_index3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "22289\n",
            "51288\n",
            "30881\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Gn-WIBrno_V"
      },
      "source": [
        "# These are random lines from the text!\n",
        "seed_text = lines[rand_index] # get random index as seed to begin training model\n",
        "\n",
        "seed_text2 = lines[rand_index2] \n",
        "\n",
        "seed_text3 = lines[rand_index3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRlu9zVbol2k"
      },
      "source": [
        "# This function allows you to print out the text from the seed_text\n",
        "\n",
        "def generate_text_seq(model, tokenizer, text_seq_length, seed_text, n_words):\n",
        "  text = []\n",
        "\n",
        "  for _ in range(n_words):\n",
        "    encoded = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    encoded = pad_sequences([encoded], maxlen = text_seq_length, truncating='pre')\n",
        "\n",
        "    y_predict = model.predict_classes(encoded)\n",
        "\n",
        "    predicted_word = ''\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "      if index == y_predict:\n",
        "        predicted_word = word\n",
        "        break\n",
        "    seed_text = seed_text + ' ' + predicted_word\n",
        "    text.append(predicted_word)\n",
        "  return ' '.join(text)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OoGc6CjjN5k"
      },
      "source": [
        "##Did not do enough data cleaning of white spaces, now the model is printing out blank spaces.... I will fix that for the summer and retrain a larger model, but this time hopefully run on Google TPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfDxk_qJrMGP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "64fbd855-5b07-4ab7-f2ca-e859c7b0942a"
      },
      "source": [
        "# Trial # 1\n",
        "generate_text_seq(model, tokenizer, seq_length, seed_text, 50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'often in the set military war animals the                                          '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AC58YJZYr6ni",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "7fb5a26a-16d2-4545-a158-349223f8c981"
      },
      "source": [
        "seed_text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'but they didnt perhaps this is for reasons that we do not understand because of our limited knowledge of their society today most likely it is because of the contingent nature of history the same contingency was probably at work when some of the societies in the middle east twelve thousand'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IezZCUJgYJa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "6c9c83f3-79ba-41f4-dd67-9c9b3f6fb3ee"
      },
      "source": [
        "# Trial # 2\n",
        "generate_text_seq(model, tokenizer, seq_length, seed_text2, 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the population of the elite been and materials the voters author                                                                                         '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIuVmP3UgffA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "90d420a7-605e-4be1-cb1f-5ea2d32a698f"
      },
      "source": [
        "seed_text2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'start of the century one in five people in spain was living in urban areas by the end this figure had halved to one in ten in a process that corresponded to increasing impoverishment of the spanish population spanish incomes fell while england grew rich the persistence and the strengthening of'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o76edkHTjBAV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "39643a81-d095-4406-db00-ded30668c072"
      },
      "source": [
        "# Trial # 3\n",
        "generate_text_seq(model, tokenizer, seq_length, seed_text3, 200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'admiral greensun relationship to lives calicoes ad the after                                                                                                                                                                                               '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HEu_gO3jBXa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "9fb51391-218d-406f-8749-7073036debdd"
      },
      "source": [
        "seed_text3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'small quantities of pollution in the atmosphere particularly the metals lead silver and copper the snow freezes and piles up on top of the snow that fell in previous years this process has been going on for millennia and provides an unrivaled opportunity for scientists to understand the extent of atmospheric'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    }
  ]
}